## 海量数据处理问题分析与解决方案

### 1. 手段
#### 1. 0 分而治之
1. **问题**：给一个超过100G大小的log file，log中存着ip地址，设计算法找到出现次数最多的ip地址。
2. **解决方案**：
    - **数据分割**：采用哈希切割与topK算法相结合的方式。将100G的大文件均匀分成1000份，利用同一个哈希函数HashFunc对每个ip地址进行计算，根据计算结果将ip映射到对应的文件中。由于哈希函数的特性，相同的ip一定会被映射到同一个文件里，且每个文件的大小可控制在能在内存中处理的范围内。
    - **局部统计**：分别处理每一个小文件，对每个文件内的ip地址进行计数统计，找出每个文件中出现次数最多的ip。具体实现时，可以使用哈希表来记录每个ip的出现次数，遍历文件中的每个ip，若哈希表中已存在该ip，则对应计数加1；若不存在，则在哈希表中新增该ip并将计数设为1。遍历结束后，哈希表中value值最大的对应的key，即为该文件中出现次数最多的ip。
    - **全局确定**：比较这1000个文件中各自出现次数最多的ip及其出现次数，从而确定整个100G文件中出现次数最多的ip。若需要找出出现次数最多的前k个ip（topK问题），则以每个小文件中出现次数最多的ip及其出现次数构建一个最小堆。构建完成后，若堆的大小超过k，就将堆顶元素（即当前堆中出现次数最少的ip）移除，直到堆的大小为k。这样，最终堆中的元素就是出现次数最多的前k个ip。

#### 1. 1 位图
1. **问题**：给定100亿整数，设计算法找到只出现一次的整数。
2. **背景知识**：一个整数通常占4个字节，100亿个整数占用的内存空间约为400亿个字节。由于1G约等于10^9个字节，所以100亿个整数大约需要40G的内存，直接存储这些整数会超出一般内存限制，因此需要使用位图及相关变形来解决。
3. **解决方案思路**：位图是一种用二进制位来表示数据状态的数据结构。对于本题，可以使用位图的变形来标记整数的出现情况。例如，对于每个整数，用2位来表示其出现次数：00表示该整数未出现，01表示出现一次，10表示出现多次，11可作他用或忽略。遍历这100亿个整数，对于每个整数对应的位图位置，若为00则置为01，表示首次出现；若为01则置为10，表示多次出现；若为10则保持不变。遍历结束后，再次扫描位图，将对应位为01的整数输出，这些就是只出现一次的整数。具体实现细节可参考：http://blog.csdn.net/qq_35402412/article/details/79415172

#### 1. 2 布隆过滤器
1. **问题**：给两个文件，分别有100亿个query，我们只有1G内存，如何找到两个文件交集？给出近似算法。
2. **解决方案**：
    - **布隆过滤器原理简述**：布隆过滤器是一种概率型数据结构，它可以高效地判断一个元素是否在一个集合中，具有空间效率高、查询速度快的特点，但存在一定的误判率（即可能把不在集合中的元素误判为在集合中）。
    - **具体操作步骤**：首先将文件A中的query进行切分，然后将切分后的query依次存入布隆过滤器。之后，读取文件B中的每个query，用这些query依次与布隆过滤器进行对比。若某个query能在布隆过滤器中找到（即布隆过滤器判断该query可能在集合A中），则认为这个query是两个文件的交集。虽然这种方法可能会有误判，但在数据量巨大且内存有限的情况下，能快速给出一个近似的交集结果。
3. **布隆过滤器常见用法**：
    - **内容去重**：在视频或文章平台中，每天会产生大量新内容。通过布隆过滤器记录用户已浏览过的内容标识，当有新内容推荐给用户时，可快速判断该内容是否已被用户浏览过，以保证用户不会看到重复内容。
    - **URL判断**：在处理海量URL数据时，判断一条URL是否在现有的URL集合之中。例如搜索引擎的爬虫程序在抓取网页时，可利用布隆过滤器快速判断某个URL是否已被抓取过，避免重复抓取。
    - **HTTP缓存优化**：对于HTTP缓存服务器，当本地局域网中的PC发起一条HTTP请求时，缓存服务器使用布隆过滤器先查看该请求的URL是否已经存在于缓存之中。若存在，则无需从原始服务器拉取数据，既能节省流量，又能加快访问速度，提升用户体验。
    - **垃圾邮件过滤**：假设邮件服务器通过发送方的邮件域或者IP地址对垃圾邮件进行过滤。邮件服务器通信邮件数量庞大时，可使用布隆过滤器快速判断当前的邮件域或者IP地址是否处于黑名单之中。详细内容可参考：https://www.cnblogs.com/tanwentao/p/15728311.html 


### 2 情景题目

#### 2.0 100 万个 IP，找出出现次数最多的 100 个（内存不同情况）
- **内存放得下时**：可以直接将所有 IP 存入内存中的数据结构（如哈希表）进行计数，然后对计数结果进行排序，取出出现次数最多的 100 个 IP。这种方式简单直接，利用哈希表的快速查找特性可以高效地完成计数操作，排序操作则可以使用如快速排序、堆排序等高效排序算法。
- **内存放不下时**：采用分治策略，将 100 万个 IP 分成若干批，每批 10 万个 IP，分别在散列表中进行计数。之后从每批计数结果中取出出现次数最多的前 10 个 IP，将这些 IP 组合起来，对相同的 IP 进行计数合并。如果合并后得到的不同 IP 数量够 100 个，直接输出；若不够，则每批再取 100 个 IP，重复上述组合、合并计数的操作，并通过快速排序得出出现次数最多的 100 个 IP。这种方法通过将大规模数据分解为小规模数据进行处理，有效降低了内存压力。

参考链接：https://blog.csdn.net/v_JULY_v/article/details/6279498

#### 2.1 8G 的 int 型数据，计算机内存只有 2G，如何排序（外部排序）
由于数据量超出内存容量，需采用外部排序方法。具体步骤如下：
1. **数据划分**：将整个 8G 文件分成许多份（设为 m 份），划分依据是每份数据大小都能放入 2G 内存。
2. **内部排序**：使用快速排序或堆排序等方法对每一份数据进行内部排序，使每份数据成为有序子串。
3. **多路归并排序**：对这 m 份有序子串进行 m 路归并排序。具体操作是，从这 m 份数据中分别取出最小元素，对这些最小元素进行排序，将排序后最小的元素输出到结果中，同时从该元素所在子串中读入一个新元素，重复此过程，直到所有数据都被输出到结果中，从而完成对 8G 数据的排序。

参考链接：https://blog.csdn.net/ailunlee/article/details/84548950

#### 2.2 海量日志数据，提取出某日访问百度次数最多的那个 IP
- **方法一**：
    - **数据提取与映射**：从海量日志数据中提取出该日且访问百度的日志中的 IP，逐个写入一个大文件。由于 IP 是 32 位，最多有 \(2^{32}\) 个 IP，采用模 1000 的映射方法，将大文件映射为 1000 个小文件。
    - **小文件统计与筛选**：对每个小文件，使用 hash_map 统计每个 IP 的出现频率，并找出每个小文件中出现频率最大的几个 IP 及相应频率。
    - **总体筛选**：在这 1000 个小文件中找出的频率最大的 IP 集合中，再次找出频率最大的那个 IP，即为所求。
- **方法二**：
    - **分治思想划分数据**：IP 地址最多有 \(2^{32} = 4G\) 种取值情况，无法完全加载到内存处理。采用“分而治之”思想，按照 IP 地址的 Hash(IP) % 1024 值，把海量 IP 日志分别存储到 1024 个小文件中，每个小文件最多包含 4MB 个 IP 地址。
    - **小文件内统计**：对于每一个小文件，构建一个以 IP 为 key，出现次数为 value 的 Hash map，同时记录当前出现次数最多的那个 IP 地址。
    - **总体排序筛选**：得到 1024 个小文件中出现次数最多的 IP 后，依据常规的排序算法得出总体上出现次数最多的 IP。

#### 2.3 统计最热门的 10 个查询串（内存不能超过 1G，一千万个记录，重复度高，去重后不超过三百万个）
- **方法一**：
    - **预处理与统计**：先对这批海量数据进行预处理，在 \(O(N)\) 的时间内利用 Hash 表完成对每个查询串的统计。
    - **借助堆结构找出 Top K**：借助堆数据结构找出 Top K。维护一个大小为 10（K 值）的小根堆，遍历 300 万的 Query，分别和根元素进行对比并调整堆。最终时间复杂度为 \(O(N) + N' * O(logK)\)，其中 \(N\) 为 1000 万（记录总数），\(N'\) 为 300 万（去重后的记录数）。
- **方法二**：采用 trie 树，关键字域存该查询串出现的次数，初始值为 0。遍历数据时更新相应查询串的出现次数，最后用 10 个元素的最小堆来对出现频率进行排序，得到最热门的 10 个查询串。

#### 2.4 1G 大小文件，每行一个词，词大小不超过 16 字节，内存限制 1M，返回频数最高的 100 个词
1. **数据划分**：顺序读取文件，对于每个词 \(x\)，取 \(hash(x) \% 5000\)，将其存到 5000 个小文件（记为 \(x0, x1,... x4999\)）中，每个文件大小约 200k 左右。若其中有文件超过 1M 大小，按照类似方法继续划分，直至所有小文件大小都不超过 1M。
2. **小文件统计与筛选**：对每个小文件，使用 trie 树或 hash_map 等结构统计每个词出现的频率，并取出出现频率最大的 100 个词（用含 100 个结点的最小堆），将这 100 个词及相应频率存入文件，得到 5000 个文件。
3. **归并操作**：对这 5000 个文件进行归并（类似归并排序），最终得到频数最高的 100 个词。

#### 2.5 对10个1G文件中的query按频度排序
- **方案1**：
    - **数据初步划分**：顺序读取10个文件，依据`hash(query) % 10`的结果，将`query`写入另外10个文件。由于`hash`函数的随机性，新生成的每个文件大小约为1G。
    - **单机统计与排序**：选取一台内存约2G的机器，使用`hash_map(query, query_count)`统计每个`query`出现的次数。随后，利用快速排序、堆排序或归并排序，依据出现次数对`query`进行排序，并将排序后的`query`及其对应的`query_count`输出到文件，得到10个排好序的文件。
    - **最终归并排序**：对这10个已排序文件进行归并排序，结合内排序与外排序，完成对所有`query`按频度的最终排序。
- **方案2**：考虑到`query`总量通常有限，只是重复次数较多，或许可一次性将所有`query`加载到内存。在此情况下，可直接采用`trie树`或`hash_map`统计每个`query`的出现次数，之后利用快速排序、堆排序或归并排序，按出现次数对`query`进行排序。
- **方案3**：类似于方案1，在通过`hash`将数据分成多个文件后，采用分布式架构（如`MapReduce`）处理这些文件。分布式处理能够充分利用多台机器的计算资源，提高处理效率。处理完成后，再对结果进行合并。

#### 2.6 找出a、b两个文件（各含50亿个url，每个url占64字节，内存限制4G）中的共同url
- **方案1**：
    - **分治划分文件**：估算每个文件大小为`50亿 × 64字节 = 320G`，远超4G内存限制，因此采用分而治之策略。遍历文件a，对每个`url`计算`hash(url) % 1000`，并依据该值将`url`存储到1000个小文件（`a0, a1,..., a999`）中，每个小文件大小约300M。对文件b执行相同操作，将`url`存储到对应的1000个小文件（`b0, b1,..., b999`）中。如此，所有可能相同的`url`都在对应的小文件对中（如`a0 vs b0`，`a1 vs b1`，...，`a999 vs b999`）。
    - **查找共同url**：针对每对小文件，将其中一个小文件的`url`存入`hash_set`，遍历另一个小文件的每个`url`，检查其是否在`hash_set`中。若存在，则该`url`是共同的`url`，将其存入结果文件。
- **方案2**：若允许一定错误率，可使用`Bloom filter`。4G内存大约可表示340亿bit。将其中一个文件中的`url`通过`Bloom filter`映射为这340亿bit，然后逐个读取另一个文件的`url`，检查其是否与`Bloom filter`匹配。若匹配，则该`url`可能是共同的`url`（但会存在一定错误率）。后续可在博客中详细阐述`Bloom filter`原理及应用。

#### 2.7 在2.5亿个整数中找出不重复的整数（内存不足以容纳这些整数）
- **方案1**：采用2 - Bitmap方法，为每个数分配2bit空间（`00`表示不存在，`01`表示出现一次，`10`表示多次，`11`无意义），共需内存`2^32 × 2 bit = 1 GB`，在可接受范围内。扫描这2.5亿个整数，查看`Bitmap`中对应位，若为`00`则变为`01`，若为`01`则变为`10`，`10`保持不变。扫描完成后，输出`Bitmap`中对应位为`01`的整数，即为不重复的整数。
- **方案2**：借鉴类似第1题思路，将数据划分成小文件处理。在每个小文件中找出不重复的整数并排序，之后进行归并操作，同时注意去除重复元素。

#### 2.8 判断一个数是否在40亿个未排序的不重复`unsigned int`整数中
- **方案1**：申请512M内存，用一个`bit`位代表一个`unsigned int`值。读入40亿个数，设置相应的`bit`位。读入要查询的数，查看相应`bit`位，若为1则表示存在，若为0则表示不存在。
- **方案2**：参考《编程珠玑》思路，由于`2^32`约为40亿多，可将40亿个数按二进制位进行划分查找。
    - **初次划分**：将40亿个数按最高位分为两类（最高位为0和最高位为1），并分别写入两个文件。其中一个文件中数的个数`<= 20亿`，另一个`>= 20亿`，与要查找数的最高位比较，进入相应文件继续查找。
    - **递归划分查找**：对进入的文件，按次最高位再次分为两类（次最高位为0和次最高位为1），同样分别写入两个文件，其中一个文件中数的个数`<= 10亿`，另一个`>= 10亿`，与要查找数的次最高位比较，继续进入相应文件查找，以此类推。这种方法时间复杂度为`O(logn)`。

此外，简单介绍位图方法：在判断整形数组是否存在重复时，若集合数据量较大，可采用位图法。按照集合中最大元素`max`创建一个长度为`max + 1`的新数组，扫描原数组，遇到数字`n`就将新数组第`n + 1`位置为1。再次遇到相同数字时，发现对应位置已为1，即表明存在重复。此方法最坏运算次数为`2N`，若事先知晓数组最大值以确定新数组长度，效率可提高一倍。


#### 2.9 海量数据中找出重复次数最多的一个
**方案1**：
 - **数据分治**：由于数据量庞大，先对海量数据进行哈希操作，然后通过求模将数据映射到多个小文件中。这一步的目的是将大规模数据分割成适合内存处理的小块，降低内存压力。
 - **小文件统计**：针对每个小文件，统计其中重复次数最多的一个元素，并记录其重复次数。在小文件内，可使用哈希表（如`hash_map`）来高效统计元素出现次数，时间复杂度为$O(n)$，其中$n$为小文件中的数据量。
 - **全局查找**：在所有小文件统计出的结果中，找出重复次数最多的那个元素，即为所求。这一步相当于在一个较小规模的数据集合中查找最值，时间复杂度取决于具体实现方式，若使用简单遍历查找，时间复杂度为$O(m)$，其中$m$为小文件统计结果的数量。整体方案时间复杂度主要取决于小文件统计步骤，为$O(N)$，$N$为海量数据总量。

#### 2.10 上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据
**方案1**：
 - **内存存储与统计**：考虑到现代机器内存通常能够容纳上千万或上亿数据，采用`hash_map`、搜索二叉树（如AVL树）或红黑树等数据结构来统计每个数据出现的次数。以`hash_map`为例，插入和查询操作平均时间复杂度为$O(1)$，因此统计所有数据出现次数的时间复杂度为$O(n)$，$n$为数据总量。
 - **堆选取Top N**：使用堆数据结构来选取出现次数最多的前$N$个数据。具体来说，维护一个大小为$N$的小顶堆，遍历统计结果，将每个数据及其出现次数与堆顶元素比较。若大于堆顶元素，则替换堆顶元素，并调整堆以保持堆的性质。这一步时间复杂度为$O(n \log N)$，因为每次调整堆的时间复杂度为$O(\log N)$。整体时间复杂度为$O(n \log N)$。

#### 2.11 一个约一万行，每行一个词的文本文件，统计最频繁出现的前10个词
**方案1**：
 - **Trie树统计**：利用Trie树统计每个词出现的次数。Trie树插入一个单词的时间复杂度与单词长度相关，假设单词平均长度为$le$，则插入$n$个单词的时间复杂度为$O(n \times le)$。
 - **堆选取Top 10**：使用堆来找出出现最频繁的前10个词。维护一个大小为10的小顶堆，遍历Trie树统计结果，将每个词及其出现次数与堆顶元素比较并调整堆。由于堆的大小固定为10，每次调整堆的时间复杂度为$O(\log 10)$，遍历所有词的时间复杂度为$O(n)$，所以这一步时间复杂度为$O(n \log 10)$。
 - **总时间复杂度**：总时间复杂度取决于$O(n \times le)$与$O(n \log 10)$中较大的一个。通常情况下，单词平均长度$le$相对较小，若$le \leq \log 10$，则总时间复杂度为$O(n \log 10)$；若$le > \log 10$，则总时间复杂度为$O(n \times le)$。

#### 2.12 100w个数中找出最大的100个数
**方案1**：
 - **最小堆实现**：构建一个含有100个元素的最小堆。遍历100w个数，每次将当前数与堆顶元素比较。若当前数大于堆顶元素，则替换堆顶元素，并调整堆以保持最小堆性质。由于每次调整堆的时间复杂度为$O(\log 100)$，遍历100w个数，所以总体时间复杂度为$O(100w \times \log 100)$。

**方案2**：
 - **快速排序思想**：采用快速排序的思路，每次分割数组后，只关注比轴值大的那一部分。当比轴值大的部分元素数量超过100时，对这部分采用传统排序算法（如插入排序、归并排序等）进行排序，然后取前100个元素。每次分割操作平均时间复杂度为$O(n)$，假设平均需要分割$k$次才能找到足够数量的较大元素，且最终传统排序的时间复杂度为$O(m \log m)$，$m$为最终需要排序的元素数量（$m \leq 100$）。因为每次分割大致将数据规模减半，所以$k \approx \log (100w)$，整体时间复杂度约为$O(100w \times \log (100w)) + O(100 \log 100)$，近似为$O(100w \times \log (100w))$，但实际中由于只关注部分数据，复杂度更接近$O(100w \times 100)$。

**方案3**：
 - **局部淘汰法**：首先选取前100个元素，并对其进行排序，记为序列$L$。然后依次扫描剩余的元素$x$，将$x$与序列$L$中最小的元素比较。若$x$比最小元素大，则删除最小元素，并利用插入排序的思想将$x$插入到序列$L$中。插入排序在已排序序列中插入一个元素的平均时间复杂度为$O(n)$，这里$n$为序列$L$的长度（固定为100），总共需要扫描$100w - 100$个元素，所以整体时间复杂度为$O((100w - 100) \times 100)$，近似为$O(100w \times 100)$。
