## 字符编码

### 1. concepts

- **计算机如何存储的？**

计算机存储数据的底层机制基于电信号的有无，以二进制的`0`和`1`来表示信息。多个`0`和`1`的组合可以表示不同的数值，无论是二进制、十六进制还是十进制，本质上都是对这些数值的不同表示形式。然而，计算机要处理文字信息，就需要人为地建立起数字与字符之间的对应关系。

- **字符集的概念**

为了让计算机能够表示文字，人们制定了不同的字符集。字符集就是规定了整数与字符之间映射关系的集合。例如，规定`61`代表字符`a`，`62`代表字符`b`等。常见的字符集有`ASCII`和`Unicode`。`ASCII`字符集主要用于支持英文文本，它的编码范围有限，无法表示中文等其他语言的字符。而`Unicode`字符集则致力于支持全球主流的所有文本，几乎涵盖了世界上所有的字符。Unicode 是一个国际标准，用于为世界上所有的字符、符号和表情符号分配一个唯一的数字。这个数字被称为字符的 Unicode 码点。Unicode 的目标是能够表示所有的写作系统，包括拉丁字母、希腊字母、阿拉伯字母、汉字、象形文字等。 Unicode 标准定义了一系列的字符集，每个字符集包含一组字符和它们对应的码点。

- **Unicode 的存储方式**

字符集仅仅定义了字符与整数的映射，并没有解决字符在计算机中实际存储的问题。以`Unicode`为例，它提供了三种主要的存储方式：

1. **UTF - 32**：采用定长编码方式，每个字符固定使用一个`uint32_t`（即 4 字节）来存储。这种方式简单直接，字符的存储和访问都很方便，每个字符占用的空间相同。但如果处理大量文本，尤其是包含许多常用英文字符的文本时，会浪费较多空间，因为英文字符实际上不需要这么大的存储空间。
2. **UTF - 16**：一个字符通常使用 1 - 2 个`uint16_t`（即 2 - 4 字节）来存储。对于基本多文种平面（BMP）内的字符，大多可以用 1 个`uint16_t`表示；而对于一些不常用的字符，可能需要 2 个`uint16_t`。这种方式在一定程度上兼顾了空间和表示能力，对于许多常见的字符，包括英文字符和部分亚洲语言字符，都能较为高效地存储。例如，一个英文字符用一个`uint16_t`存储，中文、日文等字符在 BMP 内的也用一个`uint16_t`。
3. **UTF - 8**：这是一种广泛使用的变长编码方式，一个字符用 1 - 4 个`uint8_t`（即 1 - 4 字节）存储。它根据字符的使用频率来分配存储空间，常用字符（如英文字符、数字等）用较少的字节存储，而不常用的字符（如一些罕见的符号、生僻字等）则用较多字节。一般情况下，一个英文字符用一个`uint8_t`存储，而中文、日文等字符通常需要三个`uint8_t`。这种方式在存储空间利用上较为高效，尤其适合处理多种语言混合的文本。

- **定长编码与变长编码的选择依据**

之所以会有多种存储方式，是因为不同字符在文本中出现的频率差异很大。如果所有字符出现频率相同，使用`UTF - 32`这样的定长编码是合适的，每个字符消耗相同的空间，便于处理。但在实际应用中，数字和英文字符的出现频率远高于中文、阿拉伯文等字符。为了节省存储空间，采用变长编码，让常用字符占用较少的存储空间，不常用字符占用较多空间，从而在整体上优化存储空间的使用。具体选择哪种编码方式，取决于文本的国际化程度和应用场景对空间与处理效率的权衡。

- **字符编码设计的考量因素**

  - 误码处理:在字符编码设计中，误码是一个重要的考量因素。当数据在存储或传输过程中，可能会出现单个比特（bit）的错误。一个好的编码设计应该尽量减少单个比特错误对其他数据的影响，确保即使出现误码，也不会破坏其他正常字符的数据完整性。例如，某些编码方式通过特定的校验机制或冗余信息，使得在检测到误码时能够进行一定程度的纠正或至少不影响后续字符的解析。

  - 定义域与空间利用:编码的定义域决定了它能够表示的字符范围。一般来说，定义域越大，能够表示的字符种类就越多，但同时也意味着可能需要更多的存储空间来表示每个字符。例如，`UTF - 32`能够直接表示非常广泛的字符范围，但每个字符固定占用 4 字节，相比其他编码方式在存储空间上较为浪费。在设计编码时，需要在满足应用所需字符集的前提下，平衡定义域大小和空间利用效率。

  - 开头结尾标记（变长编码）:对于变长编码，如何准确判断一个字符由几个字节组成是一个关键问题。通常的做法是在编码中添加开头结尾标记符，或者通过特定的编码规则来标识字符的边界。例如，`UTF - 8`通过字节的最高位模式来判断一个字符占用的字节数，从而实现准确的字符解析。这些标记或规则确保了在处理变长编码时，能够正确地识别每个字符的起始和结束位置，避免解析错误。

字符编码的设计是一个复杂的过程，需要综合考虑多种因素，以满足不同应用场景对字符表示、存储和处理的需求。上述只是其中一些重要的考量点，实际的编码设计还涉及到许多其他方面的细节和权衡。

### 3. 编码方案

### 3. 编码方案

#### 3.1 ASCII

ASCII（美国信息交换标准代码）是一种广泛使用的字符编码标准。

- **控制字符**：ASCII 编码的前 32 个字符（编码值从 0 到 31）被设定为控制字符。这些字符并非用于显示可见的文本内容，而是主要用于控制设备的操作。例如，换行符（ASCII 值为 10，在编程中通常表示为`\n`）用于指示文本换行，回车符（ASCII 值为 13，通常表示为`\r`）常用于将光标移动到当前行的开头，在早期的电传打字机和终端设备中起到重要的控制作用。
- **可打印字符**：ASCII 编码从 32 到 126 的这 95 个字符是可打印字符，涵盖了英文大小写字母（`a - z`、`A - Z`）、数字（`0 - 9`）、标点符号（如`.,;?!`等）以及一些特殊字符（如`$%&`等）。这些字符构成了英文文本处理的基础，使得计算机能够准确地表示和处理英文书写系统中的各种元素。
- **编码空间拓展**：ASCII 标准最初仅定义了 0 到 127 的字符集，这些字符仅需占用 7 位存储空间。然而，在实际的计算机系统和编码实现中，为了方便存储和处理，通常会使用 8 位（即一个字节）来存储每个字符。这样一来，就多出了 128 到 255 这 128 个编码值的空间。不同的系统和编码标准会利用这个额外的空间来定义更多的字符。例如，在 ISO 8859 - 1（又称 Latin - 1）编码中，128 到 255 这个范围被用于存储西欧语言中的特殊字符，像`á`、`ñ`、`ô`等，以满足这些语言在计算机中的字符表示需求。需要明确的是，这 128 到 255 的范围并不属于 ASCII 标准本身，而是由其他后续的编码标准所定义，并且在不同的编码标准中，这个范围所代表的字符集可能各不相同。

#### 3.2 UTF - 8

UTF - 8 是一种可变长度的字符编码，它在现代计算机系统和网络应用中被广泛使用，具有以下特点：

- **多字节表示**：在 UTF - 8 编码体系下，一个字符可能由 1 到 4 个字节组成。对于多字节表示的字符（即由 2、3 或 4 个字节构成的字符），除了第一个字节外，其余字节的开头两位固定为`10`。这一特性是 UTF - 8 编码用于区分多字节字符中首字节与后续字节的关键标志。例如，当遇到一个字节以`10`开头时，就知道它是某个多字节字符中的后续字节。
- **字符长度判断**：通过观察第一个字节的最高位模式，可以确定该字符的字节长度。如果第一个字节的二进制表示以`110`开头（即`110xxxxx`形式），那么这个字符是两个字节长；若以`1110`开头（`1110xxxx`形式），则该字符为三个字节长；要是以`11110`开头（`11110xxx`形式），该字符就是四个字节长。这种设计使得 UTF - 8 在处理不同字符时能够灵活地分配存储空间，同时又能准确地解析字符。
- **兼容性与适用性**：UTF - 8 编码具有出色的兼容性，它完全兼容 ASCII 编码。这意味着所有的 ASCII 字符在 UTF - 8 编码中仍然以单字节形式表示，编码值与 ASCII 编码完全相同。同时，UTF - 8 能够支持整个 Unicode 标准，从而可以表示世界上几乎所有语言的字符。在实际应用中，如果文本内容主要以英文字符为主，UTF - 8 编码大部分情况下每个字符只需一个字节，在存储空间上具有明显的优势，能够有效节省空间。然而，当处理以非英文字符为主的文本时，尤其是像中文、日文等语言，字符通常会以三字节或者四字节的形式表示。这是因为 UTF - 8 为了实现可变长度编码和字符解析的准确性，在每个字节中使用了一些比特位作为状态位，相对减少了用于表示字符本身的数据位。

#### 3.3 UTF - 16

UTF - 16 也是一种用于表示 Unicode 字符的编码方式，具有以下特性：

- **双字节与四字节表示**：UTF - 16 编码可以使用两个字节或者四个字节来表示一个字符。具体使用哪种长度取决于字符在 Unicode 字符集中的位置。
- **设计考量**：它在一定程度上是针对 UTF - 8 的一些特点进行设计的。UTF - 8 虽然在英文字符为主的场景下节省空间，但对于中文字符等其他语言字符，通常需要三个或四个字节来表示，并且由于控制位的存在，相对减少了数据位的使用。而 UTF - 16 的设计旨在优化这种情况，对于大部分常用的中文字符以及其他语言的常用字符，都可以用两个字节来表示，只有一些不常用的字符才会使用四个字节。这种编码方式在处理包含多种语言字符的文本时，在存储空间和字符处理效率之间提供了一种不同的平衡。虽然具体的编码规则较为复杂，在此不做深入研究，但总体上它为 Unicode 字符的表示提供了一种与 UTF - 8 不同的选择，适用于不同的应用场景。

#### 3.4 UTF - 32

UTF - 32 是一种固定长度的 Unicode 字符编码，其主要特点如下：

- **固定四字节表示**：UTF - 32 统一使用四个字节（32 位）来表示每一个 Unicode 字符。这种固定长度的编码方式使得字符处理变得相对简单直接。
- **字符处理优势**：由于每个字符都占用固定的 4 个字节，在处理字符串时，可以通过简单的索引操作直接访问字符串中的任何字符，无需像处理 UTF - 8 或 UTF - 16 字符串那样进行复杂的字节解析和计算。例如，要访问字符串中的第 n 个字符，只需将字符串的起始地址加上 4 \* n 的偏移量，即可直接定位到该字符的存储位置。这种特性使得 UTF - 32 在需要频繁访问或修改字符串中字符的场景下非常高效，大大简化了字符处理的逻辑。
- **空间与兼容性考量**：UTF - 32 使用的存储空间比 UTF - 8 和 UTF - 16 都要多，因为它为每个字符固定分配了四个字节。所以，当内存空间不是主要限制因素，并且应用程序需要进行大量的字符处理操作时，UTF - 32 可能是一个理想的选择。此外，在一些特定的系统或应用中，为了确保与某些特定的软件或硬件的兼容性，也可能会选择使用 UTF - 32 编码。简单概括，UTF - 32 通过固定长度编码，避免了复杂的字符解析过程，以空间换取了处理的便捷性。

### 4. std::string 和 std::wstring

设计一个`std::string`需要考虑什么？ 1.请问 string 应该用什么编码来保存? a)ANSI+:只能保存 256 种字符 b)GBK+等本地编码:失去不同 locale 下的通用性， c)UTF-8+:索引性能暴跌 d)UTF-32+:巨浪费内存 e)UTF-16+:又没索引性能，在英文字母多的时候又浪费内存

2.请问如果提供一个可以将字符串里的“坤”都替换为“鸡”的替换用方法 replace+的时候，这个方法的行为应该是 a)原地修改现有字符串 b)原字符串不变，生成一个替换过的新字符串

3.请问如果提供一个将字符串里的小写字母替换成大写字母的方法，你觉得应该考虑哪些字母? a)拉丁字母 b)希腊字母 c)西里尔字母 d)德语变音字母:ã/ e)法语变音字母:é/ z)根据当前 locale 对应的语言来决定考虑哪些字母

4.请问如果提供一个将字符串按指定分隔符分解成一个字符串列表的 split 方法，你觉得应该返回怎样的一个容器结构? a) vector b) list c)deque

如你所见，设计 string 有巨量细节需要考虑，而且很多问题是没有通用解法的。每个解法都有各自的优缺点。C++的标准库做得确实不够好，但是这一部分原因在于 C++有着极长的历史，标准库诞生的时候整个业界都还在摸索。另一方面 C++作为一个强调性能也强调抽象能力的语言，并不倾向于在一堆各有优缺点，并且没有哪个有明显优势的候选方案中强行选一个作为标准答案。所以最终的结果是 C++的标准库中的 string 类十分朴素，功能十分有限。因此当你对字符串处理有重度需求的时候，你就需要按你自己的需求去扩展 C++的标准的 string，或者自创 string 类型

在编程语言中，字符的数据类型决定了可以存储哪些字符以及如何存储这些字符。在 C 和 C++ 中，有两种主要字符类型

- char
  - 主要用于 ASCII
- wchar_t

  - 主要用于 Unicode
  - 在 Windows 平台上,`wchar_t`通常是 2 字节,使用 UTF-16 编码.当你在代码中写`wchar_t hello_w[] = L"你好你好你好";`时,`L`前缀告诉编译器这是一个宽字符字符串字面量.编译器会将这个字符串转换为 UTF-16 编码的宽字符序列,每个宽字符都是一个`wchar_t`.这个转换过程是在编译时完成的,所以它不受运行时环境的影响.
  - 需要注意的是,这个行为是特定于 Windows 平台的.在其他平台上,`wchar_t`可能是 4 字节,使用 UTF-32 编码.在这种情况下,"你"和"好"这两个字符也都会被编码为一个 4 字节的`wchar_t`.
  - 在 Windows 平台上,当你在代码中使用`wchar_t`和`L`前缀定义一个宽字符字符串时,编译器会将这个字符串转换为 UTF-16 编码的宽字符序列.

  这个转换过程是在编译时完成的,所以它不受你的输入编码格式的影响.无论你的源代码文件是使用什么编码格式保存的(例如 ASCII/UTF-8/GBK 等),编译器都会正确地解析这个文件,并将宽字符字符串转换为 UTF-16 编码. 需要注意的是,这个行为是特定于 Windows 平台的.在其他平台上,`wchar_t`可能使用其他的编码方式,例如 UTF-32.因此,如果你的代码需要在多个平台上运行,你应该避免依赖于特定的`wchar_t`编码方式. 在 Windows 平台上,当你在代码中使用`wchar_t`和`L`前缀定义一个宽字符字符串时,编译器会将这个字符串转换为 UTF-16 编码的宽字符序列. 这个转换过程是在编译时完成的,所以它不受你的输入编码格式的影响.无论你的源代码文件是使用什么编码格式保存的(例如 ASCII/UTF-8/GBK 等),编译器都会正确地解析这个文件,并将宽字符字符串转换为 UTF-16 编码. 需要注意的是,这个行为是特定于 Windows 平台的.在其他平台上,`wchar_t`可能使用其他的编码方式,例如 UTF-32.因此,如果你的代码需要在多个平台上运行,你应该避免依赖于特定的`wchar_t`编码方式.

- char16_t
  - c11 引入，用于存储 utf16
- char32_t

  - c11 引入，用于存储 utf32

- 总结 `char`和`wchar_t`的主要区别在于它们的长度和编译器对它们的处理方式. `char`通常用于存储 ASCII 字符和其他 8 位编码的字符,如 UTF-8.当你在代码中使用`char`定义一个字符串时,编译器通常不会改变字符串的编码.也就是说,如果你的源代码文件是 UTF-8 编码的,那么`char`字符串也会是 UTF-8 编码的. `wchar_t`则是一个宽字符类型,用于存储需要更多位数的字符,如 UTF-16 或 UTF-32 字符.当你在代码中使用`wchar_t`和`L`前缀定义一个宽字符字符串时,编译器会将这个字符串转换为特定的宽字符编码.在 Windows 平台上,这个编码通常是 UTF-16.

#### 4.1 std::string 和 std::wstring 有什么不同

`std::string` 和 `std::wstring` 都是 C++ 标准库中的字符串类型,但它们用于处理不同的字符集和编码.

- `std::string` 是用于处理常规字符的字符串类,它通常用于处理 ASCII 和扩展 ASCII 字符.在 `std::string` 中,每个字符通常由一个字节(8 位)表示,这取决于 `char` 类型在特定平台上的大小.

- `std::wstring` 是宽字符串类,用于处理宽字符集,如 Unicode.在 `std::wstring` 中,每个字符通常由一个 `wchar_t` 类型的值表示,这个值的大小取决于特定平台上 `wchar_t` 类型的大小.在 Windows 平台上,`wchar_t` 通常是 16 位的,可以用于表示 Unicode 中的基本多语言平面(BMP)中的字符.在一些其他平台上,`wchar_t` 可能是 32 位的,可以用于表示 Unicode 中的所有字符.

选择使用 `std::string` 还是 `std::wstring` 取决于你需要处理的字符数据的类型和编码.如果你只需要处理 ASCII 或扩展 ASCII 字符,那么 `std::string` 可能就足够了.如果你需要处理 Unicode 字符,那么你可能需要使用 `std::wstring`.

### 98. ref

1. https://142857.red/book/unicode/#_4

### 99. quiz

#### 1. char 是有符号的还是无符号的

在主流的编程平台上,`char` 的表现可能会有所不同.具体来说,`char` 是有符号的还是无符号的,取决于编译器和平台.

- **在大多数平台上,包括 Windows 和大多数 Unix-like 系统(如 Linux,macOS)上,`char` 默认是有符号的(signed).** 这意味着它可以表示从 -128 到 127 的整数.

- **然而,在某些平台上,如一些嵌入式系统和某些 Unix-like 系统(如 ARM,PowerPC),`char` 默认是无符号的(unsigned).** 这意味着它可以表示从 0 到 255 的整数.

这种差异可能会导致一些微妙的问题.例如,如果你的代码假设 `char` 是有符号的,然后在一个 `char` 是无符号的平台上运行,那么可能会出现意外的行为.因此,如果你的代码依赖于 `char` 的符号性,那么最好明确地使用 `signed char` 或 `unsigned char`.

#### 2. 为什么 char 是在 windows 平台是有符号的?

`char`在 Windows 平台上是有符号的,这主要是由于历史原因和兼容性考虑. `char`为什么是有符号的,已经无从考究了. 只能现在推测一下.

- 1. ascii 只要求了 0-127,所以满足使用要求了.
- 2. signed char 可能是 C 语言历史包袱带下来的,有符号的目的是为了做字符类型的数值运算. 例如,'b'-'a'表示距离为 1,而'a'-'b'等于-1.也许在某个古老版本依赖这种性质,因此设置为是有符号的.而 linux 的嵌入式设备受历史包袱影响较小,所以就是 unsigned char 了

#### 3. 网络传输数据的时候,为什么 byte 类型都是 unsigned char 的,signed char 可以吗?

在网络传输数据时,通常使用 `unsigned char` 来表示字节,主要有以下几个原因:

1. **统一性**:`unsigned char` 在所有平台上都表示 0 到 255 的整数,这样可以保证在不同的系统和平台之间传输数据时,数据的解释是一致的.

2. **无符号性**:网络传输的数据通常是二进制数据,而不是数值,所以没有正负之分.使用 `unsigned char` 可以避免由于符号位引起的混淆.

3. **兼容性**:许多网络协议和函数库都假定数据是由 `unsigned char` 类型的字节组成的.如果使用 `signed char`,可能会导致兼容性问题.

虽然理论上 `signed char` 也可以用于网络传输,但由于上述原因,实际上几乎所有的网络协议和函数库都使用 `unsigned char` 来表示字节.

##### 3.1 什么是无符号的兼容性?

这个问题要先关注符号扩展和零扩展.

符号扩展和零扩展是计算机科学中的两种位扩展技术,通常用于将较小的整数类型转换为较大的整数类型.

1. 符号扩展:当我们将一个有符号整数从较小的类型转换为较大的类型时,我们需要保持这个数的符号(正或负).为了做到这一点,我们将较小类型的最高位(也就是符号位)复制到较大类型的所有额外位中.例如,如果我们将一个 8 位的有符号整数-1(二进制表示为 11111111)转换为 16 位,那么结果将是-1(二进制表示为 1111111111111111).

2. 零扩展:当我们将一个无符号整数从较小的类型转换为较大的类型时,我们不需要保持符号,因为无符号整数总是正的.因此,我们可以简单地将较大类型的所有额外位设置为 0.例如,如果我们将一个 8 位的无符号整数 255(二进制表示为 11111111)转换为 16 位,那么结果将是 255(二进制表示为 0000000011111111).

这两种扩展方式都是为了保持数值的正确性.在 C++中,这些扩展通常会自动发生,例如当你将一个`char`赋值给一个`int`时.

即使传输的数据是有符号的,但是无符号也不会修改数据,不影响读.

#### 4. byte 类型用 unsigned char 好,还是 u8int_t 好?

`unsigned char` 和 `uint8_t` 都是无符号的整数类型,但是它们之间存在一些差异:

1. **大小**:`unsigned char` 的大小在不同的平台和编译器上可能会有所不同,但是它至少能够表示 0 到 255 的整数.而 `uint8_t` 是一个精确的 8 位无符号整数类型,它总是能够表示 0 到 255 的整数.

2. **可移植性**:`unsigned char` 是 C++ 标准的一部分,因此它在所有的 C++ 平台和编译器上都是可用的.而 `uint8_t` 是 C99 和 C++11 标准的一部分,因此它在一些较旧的 C++ 平台和编译器上可能不可用.

3. **用途**:`unsigned char` 通常用于表示字符或字节.而 `uint8_t` 通常用于表示精确大小的无符号整数,例如在处理二进制数据或硬件接口时.

总的来说,如果你需要一个精确的 8 位无符号整数,并且你的代码需要在 C++11 或更高版本的平台上运行,那么你应该使用 `uint8_t`.否则,你应该使用 `unsigned char`. 简单来说,unsigned char 在 c/c++标准其实没有严格限制是 8bit 的,只是要求是大于等于 8bit 的. 但是 uint8_t 又不是 c98 标准的,因此部分考虑到老代码兼容性的可能会用 unsigned char,而新的其实只需要用 uint8_t 就好了

#### 5. 对于中文来说，utf8 是三个字节，utf16 是两个字节，为什么说 utf8 和 utf16 都支持 unicode 标准呢？

简单来说 unicode 为不同字符分配了一个数字 而不管是 utf8,utf16,还是 utf32 经过转换后，都是得到同一个数字。

。Unicode 为世界上的每个字符分配了一个唯一的数字，这个数字被称为“码点”。UTF-8、UTF-16 和 UTF-32 都是 Unicode 的编码方案，它们定义了如何将这些码点转换为字节序列。

UTF-8 使用一到四个字节为每个码点编码，它是一种变长编码方式。UTF-8 的一个重要特性是它与 ASCII 编码兼容，即在 UTF-8 编码中，ASCII 字符的编码与 ASCII 完全相同。

UTF-16 使用两个字节（对于基本多语言平面的字符）或四个字节（对于辅助平面的字符）为每个码点编码。它也是一种变长编码方式，但对于大多数现代使用的字符集（包括所有的 BMP 字符），它使用两个字节编码。

UTF-32 为每个码点使用四个字节编码，它是一种定长编码方式。每个 Unicode 码点直接映射到一个四字节序列，这使得 UTF-32 编码在处理上比 UTF-8 和 UTF-16 简单，但它占用的空间更大。

### 100. 练习

```cpp
#include <iostream>
using namespace std;

int main() {
  char hello[] = "你好你好你好";
  std::cout << sizeof(hello) << std::endl;
  // 如果你好是通过utf8输入，输出19。
  // 如果你好是通过gbk输入的,输出13
  std::cout << hello << std::endl;

  wchar_t hello_w[] = L"你好你好你好";
  std::cout << sizeof(hello_w) << std::endl;
  // 输出14, 编译器转成了utf16编码。
  // 不管管"你好"输入是以utf8输入，还是其他编码方案输入。

  return 0;
}
```

- **SSO 优化**：短字符串（通常<=15 字符）直接存储在对象内部，避免堆分配
- **内存策略**：采用指数扩容（类似 vector），维护 `size`、`capacity` 和字符缓冲区
